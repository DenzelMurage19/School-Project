{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DenzelMurage19/School-Project/blob/main/%5BSample_Notebook%5D_AfterWork_Machine_Learning_with_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVHagsOlyPps"
      },
      "source": [
        "# [Sample Notebook]  AfterWork: Machine Learning with PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVd1x1O3yQy1"
      },
      "source": [
        "# Pre-requisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL-ltLJnyDhC"
      },
      "outputs": [],
      "source": [
        "# Import Pandas for data manipulation\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjLhoIkszGTD"
      },
      "outputs": [],
      "source": [
        "# Install PySpark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-Evdk3YzBuG"
      },
      "outputs": [],
      "source": [
        "# Import the PySpark library\n",
        "import pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can download the dataset for this course : https://archive.org/download/mlpy-spark/MLPySpark.zip."
      ],
      "metadata": {
        "id": "_LmpQHzQcLau"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgBYdaT2ySuL"
      },
      "source": [
        "# 1. Supervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pj_fpNzya8Y"
      },
      "source": [
        "## 1.1 Regression Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Fe0xJYty5lz"
      },
      "source": [
        "### 1.1.1 Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAtMfg5Wy7jN"
      },
      "source": [
        "We use Linear Regression to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data points. We use this technique to understand and predict the linear relationship between variables, making it a fundamental tool in predictive modeling. In real life, we can apply Linear Regression to predict house prices based on features like square footage, number of bedrooms, and location. To apply Linear Regression, we first collect and preprocess the data, then split it into training and testing sets. Next, we train a Linear Regression model on the training data using PySpark, adjusting the model parameters to minimize the error. Finally, we evaluate the model's performance on the test data to assess its predictive accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUuDDTzsyWLx"
      },
      "outputs": [],
      "source": [
        "# Data Importation and Exploration: https://afterwork.ai/ds/e/houseprices_7iht1.csv\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('HousePricesLinearRegression').getOrCreate()\n",
        "df = spark.read.csv('houseprices_7iht1.csv', header=True, inferSchema=True)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFWaQRzz7roH"
      },
      "outputs": [],
      "source": [
        "# Data Preparation (Encoding Techniques)\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer(inputCol='Neighborhood', outputCol='NeighborhoodIndex')\n",
        "df_encoded = indexer.fit(df).transform(df)\n",
        "df_encoded.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uunzx-HL8hQU"
      },
      "outputs": [],
      "source": [
        "# Machine Learning\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=['Area(sqft)', 'Bedrooms', 'Bathrooms', 'YearBuilt', 'BackyardSize', 'GarageCapacity', 'Stories', 'NeighborhoodIndex'], outputCol='features')\n",
        "output = assembler.transform(df_encoded)\n",
        "final_data = output.select('features', 'Price')\n",
        "final_data.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNBbeloJ8lT8"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "train_data, test_data = final_data.randomSplit([0.7, 0.3])\n",
        "lr = LinearRegression(featuresCol='features', labelCol='Price')\n",
        "lr_model = lr.fit(train_data)\n",
        "test_results = lr_model.evaluate(test_data)\n",
        "print('Root Mean Squared Error:', test_results.rootMeanSquaredError)\n",
        "print('R2:', test_results.r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDvaSmBv8zWr"
      },
      "outputs": [],
      "source": [
        "# Predict a New Record\n",
        "new_data = [(1800, 3, 2, 1990, 600, 2, 1, 0)]\n",
        "new_df = spark.createDataFrame(new_data, ['Area(sqft)', 'Bedrooms', 'Bathrooms', 'YearBuilt', 'BackyardSize', 'GarageCapacity', 'Stories', 'NeighborhoodIndex'])\n",
        "new_output = assembler.transform(new_df)\n",
        "prediction = lr_model.transform(new_output)\n",
        "prediction.select('prediction').show()\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj47GXAR-W0s"
      },
      "source": [
        "#### <font color=\"green\">Challenge</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF8m_VXH-ZrN"
      },
      "source": [
        "Create a simple Linear Regression model to predict house prices based on the features provided in the dataset from the URL: https://afterwork.ai/ds/ch/realestateprices_yxdp3.csv. Use PySpark to train the model and evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feRdEp3f-ZUH"
      },
      "outputs": [],
      "source": [
        "# Data Importation and Exploration: https://afterwork.ai/ds/e/realestateprices_yxdp3.csv\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('RealEstatePrices').getOrCreate()\n",
        "df = spark.read.csv('realestateprices_yxdp3.csv', header=True, inferSchema=True)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80y_Zuvc-se-"
      },
      "outputs": [],
      "source": [
        "# Data Preparation (Encoding Techniques)\n",
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoykMWX--sKQ"
      },
      "outputs": [],
      "source": [
        "# Machine Learning\n",
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYK7Z7Ta-zI8"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd6apGUy-2J9"
      },
      "outputs": [],
      "source": [
        "# Predict a New Record\n",
        "new_data = [(1800, 3, 2, 1990, 600, 2, 1, 0)]\n",
        "new_df = spark.createDataFrame(new_data, ['Area(sqft)', 'Bedrooms', 'Bathrooms', 'YearBuilt', 'BackyardSize', 'GarageCapacity', 'Stories', 'NeighborhoodIndex'])\n",
        "# Write your code here\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ewk7X7F_DKd"
      },
      "source": [
        "### 1.1.2 Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk9fh4t__WWY"
      },
      "source": [
        "In Decision Trees for Regression Analysis, we build a tree-like structure to predict continuous values. We use this technique to model the relationship between input features and a continuous target variable. Decision Trees for Regression Analysis are useful when we want to understand how different features contribute to the prediction of a continuous outcome. For example, in real estate, we can use Decision Trees for Regression Analysis to predict the selling price of a house based on features like location, size, and number of bedrooms. To apply Decision Trees for Regression Analysis, we recursively split the dataset based on feature values to create a tree structure that predicts the continuous target variable for new data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leg7VSRp_Xnz"
      },
      "outputs": [],
      "source": [
        "# Data Importation and Exploration: https://afterwork.ai/ds/e/houseprices_y206q.csv\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('HousePrices').getOrCreate()\n",
        "df = spark.read.csv('houseprices_y206q.csv', header=True, inferSchema=True)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtYiHjIf_doe"
      },
      "outputs": [],
      "source": [
        "# Data Preparation (Encoding Techniques)\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer(inputCols=['Location', 'BackyardSize', 'Pool'], outputCols=['LocationIndex', 'BackyardSizeIndex', 'PoolIndex'])\n",
        "df_encoded = indexer.fit(df).transform(df)\n",
        "df_encoded.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HPu_N7G_bBG"
      },
      "outputs": [],
      "source": [
        "# Machine Learning (Splitting, Model Training)\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=['NumBedrooms', 'NumBathrooms', 'SquareFeet', 'YearBuilt', 'NumGarageSpaces', 'LocationIndex', 'BackyardSizeIndex', 'PoolIndex'], outputCol='features')\n",
        "output = assembler.transform(df_encoded)\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "dt = DecisionTreeRegressor(featuresCol='features', labelCol='Price')\n",
        "train_data, test_data = output.randomSplit([0.7, 0.3])\n",
        "model = dt.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "260pjzkYCgji"
      },
      "outputs": [],
      "source": [
        "# Evaluation (RMSE)\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "predictions = model.transform(test_data)\n",
        "evaluator = RegressionEvaluator(labelCol='Price', predictionCol='prediction', metricName='rmse')\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print('Root Mean Squared Error (RMSE):', rmse)\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpGIL7u_eA0M"
      },
      "source": [
        "### 1.1.3 Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPWwE2gneSWr"
      },
      "source": [
        "In ensemble learning with Random Forests for regression analysis, we combine multiple decision trees to create a powerful predictive model. We use Random Forests to improve the accuracy and robustness of our regression analysis by reducing overfitting and increasing generalization. One real-life use case of Random Forests in regression is predicting housing prices based on various features like location, size, and amenities. To apply Random Forests, we first create a forest of decision trees where each tree is trained on a random subset of the data and features. During prediction, we aggregate the results from all trees to make the final regression prediction, resulting in a more reliable and accurate model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RW3xyx2meNy8"
      },
      "outputs": [],
      "source": [
        "# Data Importation and Exploration: https://afterwork.ai/ds/e/realestateprices_k9emr.csv\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('RealEstatePrices').getOrCreate()\n",
        "df = spark.read.csv('realestateprices_k9emr.csv', header=True, inferSchema=True)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "904vWnk7fkJ8"
      },
      "outputs": [],
      "source": [
        "# Data Preparation (Encoding with StringIndexer)\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexer = StringIndexer(inputCol='Location', outputCol='LocationIndex')\n",
        "df_indexed = indexer.fit(df).transform(df)\n",
        "df_indexed.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfehwVJ4iC0q"
      },
      "outputs": [],
      "source": [
        "# Data Preparation (VectorAssembler)\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "feature_cols = ['Bedrooms', 'Bathrooms', 'SquareFeet', 'LotSize(Acres)', 'YearBuilt', 'HOAFees', 'LocationIndex']\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "df_assembled = assembler.transform(df_indexed)\n",
        "df_assembled.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVy-BObqiJyp"
      },
      "outputs": [],
      "source": [
        "# Machine Learning (Model Training and Test Set Prediction)\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "rf = RandomForestRegressor(featuresCol='features', labelCol='Price')\n",
        "rf_model = rf.fit(df_assembled)\n",
        "predictions = rf_model.transform(df_assembled)\n",
        "predictions.select('Price', 'prediction').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_9AkieQiXSt"
      },
      "outputs": [],
      "source": [
        "# Evaluation (RMSE, R2)\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "evaluator = RegressionEvaluator(labelCol='Price', predictionCol='prediction', metricName='rmse')\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print('Root Mean Squared Error (RMSE):', rmse)\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol='Price', predictionCol='prediction', metricName='r2')\n",
        "r2 = evaluator.evaluate(predictions)\n",
        "print('R-squared (R2):', r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGQD1khgkGVO"
      },
      "outputs": [],
      "source": [
        "# Predict a New Record (With Pre-processed Data)\n",
        "new_data = [(2, 2, 1200, 0.5, 2010, 1300, 0)]\n",
        "new_df = spark.createDataFrame(new_data, ['Bedrooms', 'Bathrooms', 'SquareFeet', 'LotSize(Acres)', 'YearBuilt', 'HOAFees', 'LocationIndex'])\n",
        "new_assembled = assembler.transform(new_df)\n",
        "prediction_new = rf_model.transform(new_assembled)\n",
        "prediction_new.select('prediction').show()\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AwXQ2-TkSDI"
      },
      "source": [
        "#### <font color=\"green\">Challenge</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6uNJ0G-kxop"
      },
      "source": [
        "Create a Random Forest regression model to predict housing prices based on various features like location, size, and amenities using the dataset from the URL: https://afterwork.ai/ds/e/realestateprices_k9emr.csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfpGrxSKkWOc"
      },
      "outputs": [],
      "source": [
        "# Data Importation and Exploration: https://afterwork.ai/ds/e/realestateprices_k9emr.csv\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('RealEstatePrices').getOrCreate()\n",
        "df = spark.read.csv('realestateprices_k9emr.csv', header=True, inferSchema=True)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBEVU9pik1ME"
      },
      "outputs": [],
      "source": [
        "# Data Preparation (Encoding with StringIndexer)\n",
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6Noj3ZOmKpG"
      },
      "outputs": [],
      "source": [
        "# Data Preparation (VectorAssembler)\n",
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHuS75RRmOIY"
      },
      "outputs": [],
      "source": [
        "# Machine Learning (Model Training and Test Set Prediction)\n",
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G7LHSt-mPvt"
      },
      "outputs": [],
      "source": [
        "# Evaluation (RMSE, R2)\n",
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkO7kUuWmRk6"
      },
      "outputs": [],
      "source": [
        "# Predict a New Record (With Pre-processed Data)\n",
        "new_data = [(2, 2, 1200, 0.5, 2005, 1300, 0)]\n",
        "new_df = spark.createDataFrame(new_data, ['Bedrooms', 'Bathrooms', 'SquareFeet', 'LotSize(Acres)', 'YearBuilt', 'HOAFees', 'LocationIndex'])\n",
        "# Write your code here\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcmrB3NmydPi"
      },
      "source": [
        "## 1.2 Classification Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-6oEu_NnRsS"
      },
      "source": [
        "### 1.2.1 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af9E_lbMnoHB"
      },
      "source": [
        "We use Logistic Regression for binary classification tasks where we want to predict the probability of an instance belonging to a particular class. In Logistic Regression, we model the relationship between the independent variables and the probability of the target variable using the logistic function. We use this technique when we have a binary outcome, such as classifying emails as spam or not spam. For example, in email filtering, we can use Logistic Regression to predict whether an incoming email is spam or not based on features like sender, subject, and content. To apply Logistic Regression, we first preprocess the data, split it into training and testing sets, train the model using PySpark's MLlib library, evaluate the model's performance using metrics like accuracy or AUC, and make predictions on new data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKITNqtvyi0V"
      },
      "outputs": [],
      "source": [
        "# Data Importation and Exploration: https://afterwork.ai/ds/e/customer_churn_p9uk5.csv\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('CustomerChurn').getOrCreate()\n",
        "df = spark.read.csv(\"customer_churn_p9uk5.csv\", header=True, inferSchema=True)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CChAphUnror"
      },
      "outputs": [],
      "source": [
        "# Data Preparation (Encode All Categorical Variables with StringIndexer)\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+'_index', handleInvalid='skip') for col in ['Gender', 'Education', 'MaritalStatus', 'Churn']]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "df_indexed = pipeline.fit(df).transform(df)\n",
        "df_indexed.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyHPLWToofUt"
      },
      "outputs": [],
      "source": [
        "# Data Preparation (VectorAssembler)\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=['Age', 'Income', 'FamilySize', 'Usage', 'SubscriptionLength', 'Gender_index', 'Education_index', 'MaritalStatus_index'], outputCol='features')\n",
        "df_assembled = assembler.transform(df_indexed)\n",
        "df_assembled.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOc44axNojd2"
      },
      "outputs": [],
      "source": [
        "# Machine Learning (Model Training and Test Set Prediction)\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "train_data, test_data = df_assembled.randomSplit([0.7, 0.3], seed=123)\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='Churn_index')\n",
        "lr_model = lr.fit(train_data)\n",
        "predictions = lr_model.transform(test_data)\n",
        "predictions.select('Churn_index', 'prediction', 'probability').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK0DkUJdon6U"
      },
      "outputs": [],
      "source": [
        "# Evaluation (Accuracy)\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='Churn_index', predictionCol='prediction', metricName='accuracy')\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzQWtajeor1H"
      },
      "outputs": [],
      "source": [
        "# Predict a New Record (With Pre-processed Data)\n",
        "new_data = [(31, 50000, 2, 90, 12, 0.0, 1.0, 0.0)]\n",
        "new_df = spark.createDataFrame(new_data, ['Age', 'Income', 'FamilySize', 'Usage', 'SubscriptionLength', 'Gender_index', 'Education_index', 'MaritalStatus_index'])\n",
        "new_assembled = assembler.transform(new_df)\n",
        "prediction_new = lr_model.transform(new_assembled)\n",
        "prediction_new.select('prediction', 'probability').show()\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohVcVa00ndoY"
      },
      "source": [
        "### 1.2.2 Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use Naive Bayes for Text Classification to predict the category of a given text document based on the words present in it. We assume that the presence of each word is independent of the presence of other words in the document, which is a naive assumption but works well in practice. For example, we can use Naive Bayes to classify emails as spam or non-spam based on the words used in the email content. To apply Naive Bayes for text classification, we first preprocess the text data by tokenizing the words and converting them into numerical features using techniques like TF-IDF. We then train the Naive Bayes model on the labeled text data and use it to predict the category of new text documents.\n",
        "\n"
      ],
      "metadata": {
        "id": "omNCl2XtyErq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSTVM7NIndok"
      },
      "outputs": [],
      "source": [
        "# Data Importation and Exploration: https://afterwork.ai/ds/e/customer_churn_2xb5r.csv\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('CustomerChurn').getOrCreate()\n",
        "df = spark.read.csv(\"customer_churn_2xb5r.csv\", header=True, inferSchema=True)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation (Encode All Categorical Variables with StringIndexer)\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+'_index').fit(df) for col in ['Gender', 'Location', 'Subscription Type', 'Payment Method', 'Churn']]\n",
        "for indexer in indexers:\n",
        "    df = indexer.transform(df)"
      ],
      "metadata": {
        "id": "eA-_toXiyvot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation (VectorAssembler)\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=['Age', 'Monthly Charges', 'Data Usage', 'Contract Length', 'Gender_index', 'Location_index', 'Subscription Type_index', 'Payment Method_index'], outputCol='features')\n",
        "output = assembler.transform(df)"
      ],
      "metadata": {
        "id": "9dnuWMRayir8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Machine Learning (Model Training and Test Set Prediction)\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "model = NaiveBayes(labelCol='Churn_index', featuresCol='features')\n",
        "train_data, test_data = output.randomSplit([0.7, 0.3])\n",
        "nb_model = model.fit(train_data)\n",
        "predictions = nb_model.transform(test_data)"
      ],
      "metadata": {
        "id": "f3o_srq4ykIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (Accuracy, Confusion Matrix)\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='Churn_index', predictionCol='prediction', metricName='accuracy')\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "qWFffEhsz-ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict a New Record (With Pre-processed Data)\n",
        "new_data = spark.createDataFrame([(76, 110, 15, 9, 0.0, 2.0, 1.0, 0.0)], ['Age', 'Monthly Charges', 'Data Usage', 'Contract Length', 'Gender_index', 'Location_index', 'Subscription Type_index', 'Payment Method_index'])\n",
        "new_output = assembler.transform(new_data)\n",
        "prediction = nb_model.transform(new_output)\n",
        "prediction.select('prediction').show()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "pOQ_XyNS0BX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIiMDVLCndol"
      },
      "source": [
        "#### <font color=\"green\">Challenge</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Python code snippet using Naive Bayes to predict customer churn based on the provided dataset from the URL: https://afterwork.ai/ds/ch/customer_churn_ty6do.csv."
      ],
      "metadata": {
        "id": "oxup_ywv0QQR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VRl6wy9ndol"
      },
      "outputs": [],
      "source": [
        "# Data Importation and Exploration: https://afterwork.ai/ds/ch/customer_churn_ty6do.csv\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('CustomerChurn').getOrCreate()\n",
        "df = spark.read.csv(\"customer_churn_ty6do.csv\", header=True, inferSchema=True)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation (Encode All Categorical Variables with StringIndexer)\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "sELg9SYR0TZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation (VectorAssembler)\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "MkJefR1A0fUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Machine Learning (Model Training and Test Set Prediction)\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "isNYst_v2v92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (Accuracy)\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "EuL-D49S2yvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict a New Record (With Pre-processed Data)\n",
        "new_data = [(51, 1, 55, 4, 3, 70, 140, 18)]\n",
        "new_df = spark.createDataFrame(new_data, ['Age', 'Gender_index', 'Monthly Charges', 'Data Usage', 'Contract Length', 'Location_index', 'Subscription Type_index'])\n",
        "# Write your code here\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "st4kafzD211E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-s7uWUFnd3C"
      },
      "source": [
        "### 1.2.3 Support Vector Classification (SVC)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use Support Vector Classification (SVC) to classify data points into different categories by finding the hyperplane that best separates the classes. We choose SVC when we have a small to medium-sized dataset with complex decision boundaries. An example of using SVC is in image classification, where we can classify images into different categories based on their features. To apply SVC, we first preprocess the data, split it into training and testing sets, then train the SVC model on the training data. Finally, we evaluate the model's performance on the testing data to assess its classification accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "T66R8i4t31eK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbBLrbB4nd3C"
      },
      "outputs": [],
      "source": [
        "# Data Importation and Exploration: https://afterwork.ai/ds/e/customer_churn_jpeli.csv\n",
        "spark = SparkSession.builder.appName('CustomerChurnSVC').getOrCreate()\n",
        "df = spark.read.csv(\"customer_churn_jpeli.csv\", header=True, inferSchema=True)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation (Encode All Categorical Variables with StringIndexer)\n",
        "gender_indexer = StringIndexer(inputCol='Gender', outputCol='GenderIndex').fit(df)\n",
        "education_indexer = StringIndexer(inputCol='Education', outputCol='EducationIndex').fit(df)\n",
        "marital_indexer = StringIndexer(inputCol='MaritalStatus', outputCol='MaritalIndex').fit(df)\n",
        "churn_indexer = StringIndexer(inputCol='Churn', outputCol='label').fit(df)\n",
        "df_encoded = gender_indexer.transform(df)\n",
        "df_encoded = education_indexer.transform(df_encoded)\n",
        "df_encoded = marital_indexer.transform(df_encoded)\n",
        "df_encoded = churn_indexer.transform(df_encoded)\n",
        "df_encoded.show(5)"
      ],
      "metadata": {
        "id": "QH0XHs5m3pMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation (VectorAssembler)\n",
        "assembler = VectorAssembler(inputCols=['GenderIndex', 'Age', 'Income', 'FamilySize', 'EducationIndex', 'MaritalIndex', 'Usage', 'SubscriptionLength'], outputCol='features')\n",
        "output = assembler.transform(df_encoded)"
      ],
      "metadata": {
        "id": "9dSA7tPe3rsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Machine Learning (Model Training and Test Set Prediction)\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "train_data, test_data = output.randomSplit([0.7, 0.3])\n",
        "svc = LinearSVC()\n",
        "svc_model = svc.fit(train_data)\n",
        "predictions = svc_model.transform(test_data)"
      ],
      "metadata": {
        "id": "KseQWoAJ3vQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (Accuracy)\n",
        "evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "XBPoxPuc3v3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict a New Record (With Pre-processed Data)\n",
        "new_data = [(0.0, 40, 60000, 3, 1.0, 0.0, 100, 12)]\n",
        "new_df = spark.createDataFrame(new_data, ['GenderIndex', 'Age', 'Income', 'FamilySize', 'EducationIndex', 'MaritalIndex', 'Usage', 'SubscriptionLength'])\n",
        "new_output = assembler.transform(new_df)\n",
        "new_prediction = svc_model.transform(new_output)\n",
        "new_prediction.select('prediction').show()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "BNdiN6Wo3y6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8QUjZ50yWds"
      },
      "source": [
        "# 2. Unsupervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHzH7yRsykYX"
      },
      "source": [
        "## 2.1 K-Means Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In K-Means Clustering, we aim to partition a set of data points into K clusters based on their features. We use this technique to identify patterns and group similar data points together without any predefined labels. For example, we can apply K-Means Clustering to customer segmentation in marketing. By clustering customers based on their purchasing behavior, we can target specific groups with tailored marketing strategies. To apply K-Means Clustering in PySpark, we first initialize K centroids randomly, assign each data point to the nearest centroid, recalculate the centroids based on the mean of the data points in each cluster, and iterate until convergence is reached.\n",
        "\n"
      ],
      "metadata": {
        "id": "plqnPFI7KjtT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYz2NAxyYU2"
      },
      "outputs": [],
      "source": [
        "# Data Importation and Exploration: https://afterwork.ai/ds/e/customer_churn_3b8h4.csv\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('CustomerChurnKMeans').getOrCreate()\n",
        "df = spark.read.csv('customer_churn_3b8h4.csv', header=True, inferSchema=True)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation (Encode Categorical Variables with StringIndexer)\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "categorical_cols = ['Server Name', 'IP Address', 'Operating System', 'Location', 'Environment', 'Status']\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+'_index', handleInvalid='keep').fit(df) for col in categorical_cols]\n",
        "df_indexed = df\n",
        "for indexer in indexers:\n",
        "    df_indexed = indexer.transform(df_indexed)"
      ],
      "metadata": {
        "id": "CKet6iSLG8lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Machine Learning\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=['Memory (GB)', 'Storage (TB)', 'CPU Cores', 'Network Bandwidth (Gbps)',\n",
        "                                      'Server Name_index', 'IP Address_index', 'Operating System_index',\n",
        "                                      'Location_index', 'Environment_index', 'Status_index'],\n",
        "                            outputCol='features')\n",
        "df_assembled = assembler.transform(df_indexed)"
      ],
      "metadata": {
        "id": "kZQy-rlQG8ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "kmeans = KMeans(featuresCol='features', k=3)\n",
        "model = kmeans.fit(df_assembled)"
      ],
      "metadata": {
        "id": "5slsF1xkG8H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "predictions = model.transform(df_assembled)\n",
        "predictions.select('Server Name', 'prediction').show()\n",
        "\n",
        "# Evaluation with Silhouette Score\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "evaluator = ClusteringEvaluator(featuresCol='features', metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(f'Silhouette Score: {silhouette}')"
      ],
      "metadata": {
        "id": "3g1tVmZ0N5fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict a New Record (With Pre-processed Data)\n",
        "new_data = [(74, 8, 2, 0.5, 1, 1, 1, 3, 2, 1)]\n",
        "new_df = spark.createDataFrame(new_data, ['Memory (GB)', 'Storage (TB)', 'CPU Cores', 'Network Bandwidth (Gbps)',\n",
        "                                          'Server Name_index', 'IP Address_index', 'Operating System_index',\n",
        "                                          'Location_index', 'Environment_index', 'Status_index'])\n",
        "new_assembled = assembler.transform(new_df)\n",
        "new_prediction = model.transform(new_assembled)\n",
        "new_prediction.select('prediction').show()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "wjUE5pfJKWQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"green\"> Challenge </font>"
      ],
      "metadata": {
        "id": "35-xfkLoKrCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a code using PySpark to perform K-Means Clustering on the provided dataset of customer churn available at the URL: https://afterwork.ai/ds/ch/customer_churn_a4m15.csv. Use the features such as Age, Income, FamilySize, and UsageDuration for clustering the customers."
      ],
      "metadata": {
        "id": "LBa1_-_fMF7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Importation and Exploration: https://afterwork.ai/ds/ch/customer_churn_a4m15.csv\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('CustomerChurnKMeans').getOrCreate()\n",
        "df = spark.read.csv('customer_churn_a4m15.csv', header=True, inferSchema=True)\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "1-dKWzu3LFx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Machine Learning\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "z88qacPgLe7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "6svLMwR1N4Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict a New Record (With Pre-processed Data)\n",
        "new_data = [(30, 50000, 4, 60)]\n",
        "new_df = spark.createDataFrame(new_data, ['Age', 'Income', 'FamilySize', 'UsageDuration'])\n",
        "# Write your code here\n"
      ],
      "metadata": {
        "id": "VAHdxpV9Leoj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dVd1x1O3yQy1",
        "UgBYdaT2ySuL",
        "4Pj_fpNzya8Y",
        "4Fe0xJYty5lz",
        "Zj47GXAR-W0s",
        "8ewk7X7F_DKd",
        "cpGIL7u_eA0M",
        "7AwXQ2-TkSDI",
        "AcmrB3NmydPi",
        "U-6oEu_NnRsS",
        "ohVcVa00ndoY",
        "MIiMDVLCndol",
        "q-s7uWUFnd3C",
        "R8QUjZ50yWds",
        "uHzH7yRsykYX",
        "35-xfkLoKrCx"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}